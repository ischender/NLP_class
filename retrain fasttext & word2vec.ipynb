{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. I really like the Yelp dataset as a subject for machine learning and natural language processing demos, because it's big (but not so big that you need your own data center to process it), well-connected, and anyone can relate to it &mdash; it's largely about food, after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\": \"FYWN1wneV18bWNgQjJ2GNg\", \"name\": \"Dental by Design\", \"neighborhood\": \"\", \"address\": \"4855 E Warner Rd, Ste B9\", \"city\": \"Ahwatukee\", \"state\": \"AZ\", \"postal_code\": \"85044\", \"latitude\": 33.3306902, \"longitude\": -111.9785992, \"stars\": 4.0, \"review_count\": 22, \"is_open\": 1, \"attributes\": {\"AcceptsInsurance\": true, \"ByAppointmentOnly\": true, \"BusinessAcceptsCreditCards\": true}, \"categories\": [\"Dentists\", \"General Dentistry\", \"Health & Medical\", \"Oral Surgeons\", \"Cosmetic Dentists\", \"Orthodontists\"], \"hours\": {\"Friday\": \"7:30-17:00\", \"Tuesday\": \"7:30-17:00\", \"Thursday\": \"7:30-17:00\", \"Wednesday\": \"7:30-17:00\", \"Monday\": \"7:30-17:00\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "start_data_dir = '/Users/aliosha/Development/nlp/'\n",
    "data_directory = os.path.join(start_data_dir, 'data', 'yelp_dataset')\n",
    "\n",
    "businesses_filepath = os.path.join(data_directory, 'business.json')\n",
    "\n",
    "with open(businesses_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"v0i_UHJMo_hPBq9bxWvW4w\",\"user_id\":\"bv2nCi5Qv5vroFiqKGopiw\",\"business_id\":\"0W4lkclzZThpx3V65bVgig\",\"stars\":5,\"date\":\"2016-05-28\",\"text\":\"Love the staff, love the meat, love the place. Prepare for a long line around lunch or dinner hours. \\n\\nThey ask you how you want you meat, lean or something maybe, I can't remember. Just say you don't want it too fatty. \\n\\nGet a half sour pickle and a hot pepper. Hand cut french fries too.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_json_filepath = os.path.join(data_directory, 'review.json')\n",
    "\n",
    "with open(review_json_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54,618 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# open the businesses file\n",
    "with open(businesses_filepath, encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # if this business is not a restaurant, skip to the next one\n",
    "        if u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "            \n",
    "        # add the restaurant business id to our restaurant_ids set\n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "\n",
    "# turn restaurant_ids into a frozenset, as we don't need to change it anymore\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# print the number of unique restaurant ids in the dataset\n",
    "print('{:,}'.format(len(restaurant_ids)), u'restaurants in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory, 'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(intermediate_directory):\n",
    "    os.makedirs(intermediate_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 50,000 restaurant reviews\n",
      "              written to the new txt file.\n",
      "CPU times: user 1.28 s, sys: 156 ms, total: 1.43 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_rev = 50000\n",
    "# set process to False if you don't want to process the data\n",
    "process = True\n",
    "# process = False\n",
    "\n",
    "review_count = 0\n",
    "\n",
    "if process:\n",
    "    with open(review_txt_filepath, 'w+', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        with open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                # if this review is not about a restaurant, skip to the next one\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                # write the restaurant review as a line in the new file\n",
    "                # escape newline characters in the original review text\n",
    "                review_txt_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "                if review_count >= max_rev:\n",
    "                    break\n",
    "\n",
    "    print(u'''Text from {:,} restaurant reviews\n",
    "              written to the new txt file.'''.format(review_count))\n",
    "    \n",
    "else:\n",
    "    with open(review_txt_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print(u'Text from {:,} restaurant reviews in the txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aliosha/Development/nlp/data/yelp_dataset/intermediate/review_text_all.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_txt_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "data = LineSentence(review_txt_filepath)\n",
    "\n",
    "model_gensim = FastText(size=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model_gensim.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliosha/Envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=13161, size=100, alpha=0.025)\n",
      "CPU times: user 1min 44s, sys: 1.03 s, total: 1min 45s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train the model\n",
    "model_gensim.train(data, total_examples=model_gensim.corpus_count, epochs=model_gensim.iter)\n",
    "\n",
    "print(model_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=32586, size=100, alpha=0.025)\n",
      "CPU times: user 7.2 s, sys: 917 ms, total: 8.12 s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.wrappers.fasttext import FastText as FT_wrapper\n",
    "\n",
    "# Set FastText home to the path to the FastText executable\n",
    "ft_home = '/Users/aliosha/Development/nlp/fastText/fasttext'\n",
    "\n",
    "# train the model\n",
    "model_wrapper = FT_wrapper.train(ft_home, review_txt_filepath)\n",
    "\n",
    "print(model_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=13161, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# saving a model trained via Gensim's fastText implementation\n",
    "model_gensim.save('saved_model_gensim')\n",
    "loaded_model = FT_gensim.load('saved_model_gensim')\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=32586, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# saving a model trained via fastText wrapper\n",
    "model_wrapper.save('saved_model_wrapper')\n",
    "loaded_model = FT_wrapper.load('saved_model_wrapper')\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "[-2.7185776   1.4365803  -1.6437114   2.0584633  -1.4260191  -4.0511084\n",
      " -0.9091086   0.8413393   0.39338198 -0.24548973 -1.1307178   0.52726805\n",
      "  0.8045797  -0.68693244 -1.812846    2.205074    3.255425    1.0463287\n",
      "  1.4344546  -0.12507983  0.09863514  3.122248   -1.0183789  -1.6110741\n",
      "  2.0402436  -0.2760119   2.0748363  -0.84881026  0.27457434  1.6650326\n",
      " -0.7523171  -0.71506804  1.4569962   3.4606004  -0.08815765  1.7500066\n",
      "  3.950676    1.4748651   2.8572576   2.6455152   4.744443   -3.354194\n",
      "  2.1705937  -2.9115553  -2.9871023   0.89958733 -1.0711738   3.0827706\n",
      "  1.8265618   3.1305425  -2.5459406  -0.502866    1.7525264   0.58584493\n",
      " -1.4412364  -0.24747092  1.7170135   1.5159978   3.5333445  -0.40977845\n",
      "  0.37035015 -0.71257204 -2.1066566  -1.9206173   0.9679985  -0.6458595\n",
      " -3.1274252   0.85532665  0.19122182  0.06846409 -1.3375468   0.16227752\n",
      " -0.20098454  0.42042786  2.0671573  -1.1599432   2.0633118   1.1976726\n",
      "  0.71644056 -1.7480453  -1.755339    0.12139568  0.5296901  -0.5787521\n",
      "  0.12532592 -1.6313198  -0.3519818  -0.66290635  4.165934   -2.8627982\n",
      "  4.198055    3.609181    1.5352968   1.4954697  -0.8748884  -0.73688483\n",
      "  0.1502165   0.7909      3.017471    1.5686184 ]\n",
      "[-2.064974    0.27182603 -1.389041    1.2521557  -1.5592729  -3.6774566\n",
      " -1.2961239  -0.11608142  0.15217562  0.12256394 -1.2560822   0.84227353\n",
      "  0.05720048  0.6441937  -1.4980094   0.9956759   2.5910425   1.1898502\n",
      "  0.08785147 -0.09231465  0.83599675  2.7255228  -0.4264073  -0.7465346\n",
      "  1.2681869   0.05535352  1.4193418  -0.26705173 -0.82159823  1.7122189\n",
      " -1.0731802  -0.06288622  1.2803319   2.1653855  -0.4228908   0.8406909\n",
      "  2.993371    1.1298894   1.3145466   2.467583    3.1882882  -1.3004265\n",
      "  0.7532726  -1.5395061  -3.1357696   0.577545   -0.82898635  2.7096298\n",
      "  1.871646    2.4711666  -1.9035028  -1.6397724   1.1660907   0.6571501\n",
      " -0.93836707  0.6146131   1.4814152   1.0115784   2.248303   -1.1900451\n",
      "  0.99832946 -0.13469517 -2.1822925  -0.5403867   1.3713465  -1.5229502\n",
      " -2.5473871   1.1384119  -0.40399507 -0.91489613 -0.43204424  0.16834898\n",
      " -0.3152025  -0.2895613   2.225082   -1.4605162  -0.5882522   0.73385817\n",
      "  0.5911282  -0.7361801  -0.70346767  0.3309105   1.160136   -0.96593255\n",
      "  0.47953862 -0.89455646  0.6866212  -0.9285763   3.6697512  -2.315381\n",
      "  2.1377122   3.6553457   0.9976912   0.30876604 -0.18612097 -0.57929707\n",
      " -0.14418884 -0.26398024  1.8332626   0.70904016]\n"
     ]
    }
   ],
   "source": [
    "print('night' in model_wrapper.wv.vocab)\n",
    "print('nights' in model_wrapper.wv.vocab)\n",
    "print(model_wrapper['night'])\n",
    "print(model_wrapper['nights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9151657960448711"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.similarity(\"night\", \"nights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheeseburger', 0.9603231549263),\n",
       " ('cheeseburger,', 0.9294769763946533),\n",
       " ('Cheeseburger', 0.9247754812240601),\n",
       " ('burger\"', 0.914174497127533),\n",
       " ('burger)', 0.9139341711997986),\n",
       " ('Cheeseburger,', 0.9134862422943115),\n",
       " ('burger?', 0.9113364219665527),\n",
       " ('burger:', 0.9105531573295593),\n",
       " ('cheeseburgers', 0.9074652194976807),\n",
       " ('burger!', 0.8963555097579956)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.most_similar(\"cheesburger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1, model=model_wrapper):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = model.wv.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print( term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheeseburger\n"
     ]
    }
   ],
   "source": [
    "word_algebra(['cheese', 'burger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breakfast/brunch\n",
      "lunch!\n",
      "lunch,\n",
      "Smunch\n",
      "Lunch\n",
      "lunch/dinner\n",
      "brunch\n"
     ]
    }
   ],
   "source": [
    "word_algebra(['lunch', 'breakfast'], topn=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunch!\n",
      "lunch,\n",
      "brunch\n",
      "Lunch\n",
      "lunch/dinner\n",
      "brunch,\n",
      "lunch.\n"
     ]
    }
   ],
   "source": [
    "word_algebra(['lunch', 'night'], subtract=['day'], topn=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunch!\n",
      "brunch\n",
      "brunch,\n",
      "lunch,\n",
      "Lunch\n",
      "lunch/dinner\n",
      "Brunch\n"
     ]
    }
   ],
   "source": [
    "word_algebra(['lunch', 'night'], subtract=['day'], topn=7, model=model_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# initiate the model and perform the first epoch of training\n",
    "word2vec = Word2Vec(data, size=100, window=5,\n",
    "                    min_count=20, sg=1, workers=4)\n",
    "\n",
    "word2vec.save(word2vec_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 training epochs so far.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# perform another 11 epochs of training\n",
    "for i in range(1,12):\n",
    "    word2vec.train(data, total_examples=word2vec.corpus_count, epochs=word2vec.epochs)\n",
    "    word2vec.save(word2vec_filepath)\n",
    "        \n",
    "\n",
    "print(u'{} training epochs so far.'.format(word2vec.train_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the finished model from disk\n",
    "word2vec = Word2Vec.load(word2vec_filepath)\n",
    "word2vec.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brunch\n",
      "breakfast,\n",
      "breakfast.\n",
      "lunch,\n",
      "lunch.\n",
      "dinner\n",
      "weekday\n"
     ]
    }
   ],
   "source": [
    "word_algebra(['lunch', 'breakfast'], topn=7, model=word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinner\n",
      "lunch,\n",
      "Wednesday\n",
      "lunch.\n",
      "Friday\n",
      "weeknight\n",
      "evening\n"
     ]
    }
   ],
   "source": [
    "word_algebra(['lunch', 'night'], subtract=['day'], topn=7, model=word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
